
[News: new code avaible https://anonymous.4open.science/r/MedicalRagChecker-752E/README.md]
# MRAGChecker

> A generalized, backendâ€‘agnostic **RAG & Multimodal** checker that works with **OpenAI GPT**, **Amazon Bedrock (Claude)**, and **openâ€‘source models via vLLM**. Split into two pluggable parts:
>
> 1) **Textâ€‘only RAGChecker** (compatible with the classic RAG checking workflow)
> 2) **MRAG (Multimodal) Checker** for imageâ€‘text QA and chart/slide/doc VQA

All code comments in this repo are in **English**.


# running process

1. `python medical_data/make_rag_inputs.py   --datasets liveqa pubmedqa medquad   --split train   --limit 50   --out-dir tests/_min_input`

2. `CUDA_VISIBLE_DEVICES=0 python rag/generate_with_vllm.py   --in-jsonl tests/_min_input/rag_generation_outputs_liveqa.jsonl   --out-jsonl tests/_min_input/rag_generation_outputs_liveqa_qwen.jsonl   --backend qwen   --gpu-util 0.5   --max-len 6144   --max-new-tokens 256   --temperature 0.2`

3. 
 `python smokeopenai.py   --eval-backend gpt-4o-mini   --input-dir tests/_min_input   --output-dir tests/_min_output   --limit 10   --claims-jsonl tests/_min_output/claims_dump.jsonl`

---

## âœ¨ Why this repo?
- Keep the familiar workflow and JSON schemas from prior â€œragcheckerâ€ scripts.
- Remove vendor lockâ€‘in: use **GPT**, **Claude**, **or** open models via **vLLM** with one unified interface.
- Support both **textâ€‘only** and **multimodal** (image + text) cases.
- Provide a clean **CLI** and **Python API** you can embed in your own pipelines.

---

## ğŸ§± Repository layout
```
MRAGChecker/
â”œâ”€ pyproject.toml                      # or setup.cfg / setup.py
â”œâ”€ README.md                           # this file
â”œâ”€ mragchecker/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ schema.py                        # dataclasses for I/O items
â”‚  â”œâ”€ io_utils.py                      # jsonl read/write, packing/unpacking
â”‚  â”œâ”€ prompts/
â”‚  â”‚  â”œâ”€ prompt_builder.py             # assemble prompts consistently
â”‚  â”‚  â”œâ”€ judge_templates.py            # LLM-as-judge templates
â”‚  â”œâ”€ metrics/
â”‚  â”‚  â”œâ”€ text_metrics.py               # EM/F1/etc for text QA
â”‚  â”‚  â”œâ”€ mcq_metrics.py                # accuracy, letter extraction, etc.
â”‚  â”‚  â””â”€ mrag_metrics.py               # multimodal extras (PED, path stats)
â”‚  â”œâ”€ pipelines/
â”‚  â”‚  â”œâ”€ generate.py                   # run generation over datasets
â”‚  â”‚  â”œâ”€ evaluate.py                   # run judge LLM + compute metrics
â”‚  â”‚  â””â”€ pack_for_checker.py           # pack your jsonl -> checking_inputs.json
â”‚  â”œâ”€ datasets/
â”‚  â”‚  â”œâ”€ webqa.py                      # loaders/adapters (text+image ids)
â”‚  â”‚  â”œâ”€ visrag.py                     # Arxiv/Plot/Slide/Doc adapters
â”‚  â”‚  â”œâ”€ chartrag.py                   # Chart-MRAG adapter
â”‚  â”‚  â””â”€ mrag_bench.py                 # MRAG-Bench adapter
â”‚  â””â”€ backends/
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ base.py                       # ChatModel interface (text/vision)
â”‚     â”œâ”€ openai_backend.py             # GPT via OpenAI Responses API
â”‚     â”œâ”€ bedrock_backend.py            # Claude via Bedrock Runtime
â”‚     â””â”€ vllm_backend.py               # open-source models via vLLM
â”œâ”€ scripts/
â”‚  â”œâ”€ convert_to_ragchecker_input.py   # convenience packer
â”‚  â””â”€ demo_quickstart.sh               # end-to-end runnable example
â””â”€ examples/
   â””â”€ checking_inputs.json             # sample packed input for evaluator
```

---

## ğŸ”Œ Unified model interface

Create a thin abstract interface so every backend behaves the same at callâ€‘sites.

```python
# mragchecker/backends/base.py
# NOTE: Comments in English only.
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional, Union
from PIL import Image

@dataclass
class GenerateConfig:
    max_tokens: int = 512
    temperature: float = 0.2

class ChatModel:
    """Unified interface for text or multimodal generation."""
    name: str

    def supports_vision(self) -> bool:
        return False

    def generate(
        self,
        prompt_text: str,
        retrieved_texts: Optional[List[str]] = None,
        image_inputs: Optional[List[Union[str, Image.Image]]] = None,
        cfg: Optional[GenerateConfig] = None,
    ) -> str:
        raise NotImplementedError
```

### OpenAI (GPT)
```python
# mragchecker/backends/openai_backend.py
from .base import ChatModel, GenerateConfig
# Implement using OpenAI Responses API; encode images as data URLs.
# generate() builds a single user turn with optional context + images + prompt.
```

### Bedrock (Claude)
```python
# mragchecker/backends/bedrock_backend.py
from .base import ChatModel, GenerateConfig
# Implement via boto3 bedrock-runtime; images in Anthropic input_image format.
```

### vLLM (open-source models)
```python
# mragchecker/backends/vllm_backend.py
from .base import ChatModel, GenerateConfig
# Wrap vLLM LLM + SamplingParams; format prompts per model family (llava/qwen/phi...).
```

> Each backend exposes: `supports_vision()` and `generate(...)`.

---

## ğŸ“¦ I/O schema (compatible with RAGChecker-style JSON)

```python
# mragchecker/schema.py
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any

@dataclass
class ContextDoc:
    doc_id: str
    text: str

@dataclass
class Sample:
    dataset: str
    query_id: str
    query: str
    gt_answer: str | Dict[str, Any] | None
    options: List[str] = field(default_factory=list)  # MCQ options if any
    images: List[str] = field(default_factory=list)   # local paths or URIs
    retrieved_context: List[ContextDoc] = field(default_factory=list)

@dataclass
class GenerationResult:
    sample: Sample
    response: str
    pred_letter: Optional[str] = None
    correct: Optional[bool] = None
    meta: Dict[str, Any] = field(default_factory=dict)
```

JSONL rows for generation cache mirror `GenerationResult` (flattened). A `checking_inputs.json` packs all rows under a topâ€‘level `{ "results": [...] }` for the evaluator.

---

## ğŸ§ª Pipelines

### Generation
`mragchecker/pipelines/generate.py`

```python
# Pseudocode outline (comments only in English)
from .schema import Sample, GenerationResult
from ..backends.base import ChatModel, GenerateConfig
from ..prompts.prompt_builder import assemble_prompt, extract_answer_letter

def run_generation(samples: list[Sample], model: ChatModel, cfg: GenerateConfig) -> list[GenerationResult]:
    out = []
    for s in samples:
        prompt, pm = assemble_prompt(s)  # returns prompt_text and prompt_meta
        resp = model.generate(prompt_text=prompt,
                              retrieved_texts=[d.text for d in s.retrieved_context],
                              image_inputs=s.images,
                              cfg=cfg)
        letter = extract_answer_letter(resp) if s.options else None
        correct = (letter == s.gt_answer) if (letter and isinstance(s.gt_answer, str) and len(s.gt_answer) == 1) else None
        out.append(GenerationResult(sample=s, response=resp, pred_letter=letter, correct=correct, meta={"prompt_meta": pm}))
    return out
```

### Evaluate (LLMâ€‘asâ€‘Judge + metrics)
`mragchecker/pipelines/evaluate.py`

```python
# Outline: run a judge model with a template and compute metrics.
# Supports both text-only and multimodal (images optional).
```

### Pack to RAGChecker input
`mragchecker/pipelines/pack_for_checker.py` or `scripts/convert_to_ragchecker_input.py`

---

## ğŸ› ï¸ Datasets/adapters
Each adapter returns a list of `Sample` items with `query`, `gt_answer`, optional `options`, optional `images`, and `retrieved_context` (after calling your retriever).

- `webqa.py`: text + images, context facts by id.
- `visrag.py`: Arxiv/Plot/Slide/Doc â€“ convert HF splits into `Sample` (with single image for slide/doc; multiple for plot if needed).
- `chartrag.py`: build `images` list and get `gt_answer` from Chart-MRAG.
- `mrag_bench.py`: populate `options` (A/B/C/D) and the MCQ ground truth letter.

Adapters are **pure data**: no generation inside. This keeps the pipeline testable.

---

## ğŸš€ Quickstart

### Install
```bash
pip install -e .
# or: pip install mragchecker  # when published
```

### Env (choose any backend)
```bash
# GPT (OpenAI)

export OPENAI_MODEL=gpt-4o            # or your Azure deployment name
# export OPENAI_BASE_URL=...          # Azure / OpenAI-compatible proxy

# Claude (Bedrock)
export AWS_REGION=us-east-1
# ensure your AWS credentials are configured (env or ~/.aws/credentials)

# vLLM (open-source)
# ensure vLLM is installed and GPU is available
```

### Run a tiny demo (text-only, GPT)
```bash
mragcheck gen \
  --dataset webqa --limit 16 \
  --backend gpt \
  --out runs/gpt/webqa.jsonl

mragcheck pack \
  --in-dir runs/gpt \
  --out examples/checking_inputs.json

mragcheck eval \
  --inputs examples/checking_inputs.json \
  --judge gpt \
  --metrics all \
  --out runs/gpt/checking_outputs.json
```

### Run a tiny demo (multimodal, vLLM llava)
```bash
mragcheck gen \
  --dataset visrag_arxiv --limit 8 \
  --backend llava \
  --out runs/llava/visrag_arxiv.jsonl

mragcheck pack --in-dir runs/llava --out examples/checking_inputs.json
mragcheck eval --inputs examples/checking_inputs.json --judge gpt --metrics all --out runs/llava/check.json
```

> The CLI is a thin wrapper over the pipeline functions. Internally it builds a `ChatModel` based on `--backend`.

---

## ğŸ”§ CLI design (argparse or Typer)
```
mragcheck gen   --dataset {webqa,visrag_arxiv,visrag_plot,visrag_slide,visrag_doc,chartrag,mrag} \
                --backend {gpt,claude,llava,qwen,phi,pixtral,internvl3,mplug,deepseek} \
                [--limit N] [--max-tokens 512] [--temperature 0.2] \
                [--fewshot FILE.json] [--init-style ...] [--reasoning-process ...] \
                --out OUT.jsonl

mragcheck pack  --in-dir RUN_DIR --out checking_inputs.json

mragcheck eval  --inputs checking_inputs.json \
                --judge {gpt,claude,llava,qwen,...} \
                --metrics {all,text_only,multimodal,mcq} \
                --out checking_outputs.json
```

---

## ğŸ”„ Migration guide from your current repo

1) **Create the package skeleton** using the layout above.
2) **Move your existing retrieval + prompt assembly** into `mragchecker/prompts/prompt_builder.py` and dataset adapters under `mragchecker/datasets/`.
3) **Replace adâ€‘hoc model calls** with `ChatModel` instances:
   - `OpenAIChatModel` (GPT)
   - `BedrockClaudeModel`
   - `VLLMModel` (configurable family key: llava/qwen/phi/...)
4) **Replace your `run_filter_rag_checker_prompt.py`** with the CLI entry `mragcheck gen` or keep it as a legacy script that just calls `pipelines.generate.run_generation`.
5) **Keep MRAGâ€‘specific fields**: for MCQ, include `options` and keep your `extract_answer_letter` logic in `metrics/mcq_metrics.py`.
6) **Pack & evaluate** using the provided packer and evaluator, preserving the original input schema expected by your downstream RAGChecker flow.

> Minimal code edits in your legacy script:
> - Import `from mragchecker.backends import OpenAIChatModel, BedrockClaudeModel, VLLMModel` (or factory `build_model(kind=...)`).
> - Build `Sample` objects from your loaders.
> - Call `run_generation(samples, model, cfg)` and write JSONL.

---

## ğŸ” Configuration
- Most secrets via environment variables (`OPENAI_API_KEY`, `OPENAI_BASE_URL`, `AWS_REGION`, etc.).
- Optional YAML/JSON presets to map `--backend llava` â†’ HF repo id; stored in `configs/backends.yaml`.

---

## ğŸ“ Metrics (starter set)
- **Text**: EM, tokenâ€‘F1, BLEU (optional), Rougeâ€‘L (optional)
- **MCQ**: accuracy, letter extraction quality
- **Multimodal**: retrieval hit rate (if you log doc ids), **Path Edit Distance (PED)** vs a canonical path DSL (optional), step counts, correctionâ€‘afterâ€‘error rate.

> PED idea: define your ideal path in a lightweight DSL (`open(url) -> find(selector) -> click(id) -> extract(field)`), then compute edit distance between the system path and canonical path.

---

## ğŸ§© Extending backends
Add a new class implementing `ChatModel` and register it in a small factory:

```python
# mragchecker/backends/__init__.py
from .openai_backend import OpenAIChatModel
from .bedrock_backend import BedrockClaudeModel
from .vllm_backend import VLLMModel

def build_model(kind: str, **kwargs):
    kind = kind.lower()
    if kind in {"gpt", "openai"}:
        return OpenAIChatModel(**kwargs)
    if kind in {"claude", "bedrock"}:
        return BedrockClaudeModel(**kwargs)
    return VLLMModel(kind=kind, **kwargs)  # llava/qwen/phi/... via vLLM
```

---

## ğŸ“ License
Choose a permissive license (MIT/Apacheâ€‘2.0) unless you have constraints. Note that some model weights (and APIs) have their own terms.

---

## ğŸ¤ Contributing
- Keep comments in English.
- Add dataset adapters under `mragchecker/datasets/`.
- Add metrics in `mragchecker/metrics/` with unit tests.
- Backends should remain stateless (or lightly cached) and configured only via `GenerateConfig`/env.

---

## ğŸ“š Roadmap
- Add batching and streaming support.
- Add structured judge outputs and confidence scoring.
- Add trace logging (prompt hash, model config snapshot) for full reproducibility.
- Optional web UI for browsing runs.
